{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "MRR1 = []\n",
    "MRR2 = []\n",
    "MRR3 = []\n",
    "pd.options.mode.chained_assignment = None\n",
    "df = pd.read_csv('C:/Users/Luca Eigenbau/Documents/DesktUp/Uni/BSP/yagoFactsCleaned.csv')\n",
    "df.columns = ['Subject', 'Predicate', 'Object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is similar to the code in \"yagoFactsSearchEngine.py\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queryList, goldenTriple):\n",
    "    rpr = 0\n",
    "    mainIndexList = []\n",
    "    importance = []\n",
    "    predicateListFrequencies = []\n",
    "    for i in range(0, len(queryList)):\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Subject\"] == queryList[i]].tolist())\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Predicate\"] == queryList[i]].tolist())\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Object\"] == queryList[i]].tolist())\n",
    "    mainIndexListUnique = list(dict.fromkeys(mainIndexList))\n",
    "    for m in mainIndexListUnique:\n",
    "        importance.append(100**mainIndexList.count(m))\n",
    "    df2 = df.iloc[mainIndexListUnique]\n",
    "    df2.loc[:, \"Relevance\"] = importance\n",
    "    predicateList = df2.loc[:, 'Predicate'].tolist()\n",
    "    predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "    for k in predicateList:\n",
    "        predicateListFrequencies.append(predicateList.count(k))\n",
    "    lowestUniquePredicateFrequency = min(predicateListFrequencies)\n",
    "    for n in predicateListUnique:\n",
    "        df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance'] = (df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance']/predicateList.count(n)) * lowestUniquePredicateFrequency\n",
    "    df2 = df2.sort_values(by=['Relevance'], ascending=False)\n",
    "    if len(df2) >= 10:\n",
    "        for p in range(0, len(df2)):\n",
    "            if df2.iloc[p][\"Subject\"] == goldenTriple[0] and df2.iloc[p][\"Predicate\"] == goldenTriple[1] and df2.iloc[p][\"Object\"] == goldenTriple[2]:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    else:\n",
    "        for p in range(0, 10):\n",
    "            if df2.iloc[p][\"Subject\"] == goldenTriple[0] and df2.iloc[p][\"Predicate\"] == goldenTriple[1] and df2.iloc[p][\"Object\"] == goldenTriple[2]:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    return rpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean reciprocal rank (MRR) of this search function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35900429088611807\n",
      "Wall time: 4min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MRR1.append(search(['Rocky_Johnson', 'Dwayne_Johnson'], ['Rocky_Johnson', 'hasChild', 'Dwayne_Johnson']))\n",
    "MRR1.append(search(['Rocky_Johnson', 'hasChild'], ['Rocky_Johnson', 'hasChild', 'Dwayne_Johnson']))\n",
    "MRR1.append(search(['Roman_Empire', 'hasCurrency'], ['Roman_Empire', 'hasCurrency', 'Sestertius']))\n",
    "MRR1.append(search(['Rome', 'http://www.comune.roma.it/'], ['Rome', 'hasWebsite', 'http://www.comune.roma.it/']))\n",
    "MRR1.append(search(['directed', 'San_Andreas_(film)'], ['Brad_Peyton', 'directed', 'San_Andreas_(film)']))\n",
    "MRR1.append(search(['wroteMusicFor', 'Cosmopolitan_(film)'], ['Andrew_Lockington', 'wroteMusicFor', 'Cosmopolitan_(film)']))\n",
    "MRR1.append(search(['Kiribati', 'hasCapital'], ['Kiribati', 'hasCapital', 'South_Tarawa']))\n",
    "MRR1.append(search(['Charles_the_Fat', 'East_Francia'], ['Charles_the_Fat', 'wasBornIn', 'East_Francia']))\n",
    "MRR1.append(search(['hasChild', 'Michelle_Obama'], ['Marian_Shields_Robinson', 'hasChild', 'Michelle_Obama']))\n",
    "MRR1.append(search(['Greenland', 'hasCurrency'], ['Greenland', 'hasCurrency', 'Danish_krone']))\n",
    "MRR1 = statistics.mean(MRR1)\n",
    "print(MRR1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MRR is **worse than expected** and the search function takes a lot of time. **Fixing the issues requires changing the ranking algorithm** to pay more attention to predicates in selected rows, instead of letting the function select all rows with a matching predicate and ranking those. The main changes in the following function are marked with comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queryList, goldenTriple):\n",
    "    rpr = 0\n",
    "    mainIndexList = []\n",
    "    importance = []\n",
    "    predicateListFrequencies = []\n",
    "    for i in range(0, len(queryList)):\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Subject\"] == queryList[i]].tolist()) # Omitting the search for\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Object\"] == queryList[i]].tolist()) # rows with matching predicate\n",
    "    mainIndexListUnique = list(dict.fromkeys(mainIndexList))\n",
    "    for m in mainIndexListUnique:\n",
    "        if df.iloc[m][\"Predicate\"] in queryList:\n",
    "            importance.append(100**mainIndexList.count(m)*10) # Adding value to rows with matching predicates\n",
    "        else:\n",
    "            importance.append(100**mainIndexList.count(m))\n",
    "    df2 = df.iloc[mainIndexListUnique]\n",
    "    df2.loc[:, \"Relevance\"] = importance\n",
    "    predicateList = df2.loc[:, 'Predicate'].tolist()\n",
    "    predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "    for k in predicateList:\n",
    "        predicateListFrequencies.append(predicateList.count(k))\n",
    "    lowestUniquePredicateFrequency = min(predicateListFrequencies)\n",
    "    for n in predicateListUnique:\n",
    "        df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance'] = (df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance']/predicateList.count(n)) * lowestUniquePredicateFrequency\n",
    "    df2 = df2.sort_values(by=['Relevance'], ascending=False)\n",
    "    if len(df2) >= 10:\n",
    "        for p in range(0, len(df2)):\n",
    "            if df2.iloc[p][\"Subject\"] == goldenTriple[0] and df2.iloc[p][\"Predicate\"] == goldenTriple[1] and df2.iloc[p][\"Object\"] == goldenTriple[2]:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    else:\n",
    "        for p in range(0, 10):\n",
    "            if df2.iloc[p][\"Subject\"] == goldenTriple[0] and df2.iloc[p][\"Predicate\"] == goldenTriple[1] and df2.iloc[p][\"Object\"] == goldenTriple[2]:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    return rpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MRR of this function with the exact same benchmark queries is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MRR2.append(search(['Rocky_Johnson', 'Dwayne_Johnson'], ['Rocky_Johnson', 'hasChild', 'Dwayne_Johnson']))\n",
    "MRR2.append(search(['Rocky_Johnson', 'hasChild'], ['Rocky_Johnson', 'hasChild', 'Dwayne_Johnson']))\n",
    "MRR2.append(search(['Roman_Empire', 'hasCurrency'], ['Roman_Empire', 'hasCurrency', 'Sestertius']))\n",
    "MRR2.append(search(['Rome', 'http://www.comune.roma.it/'], ['Rome', 'hasWebsite', 'http://www.comune.roma.it/']))\n",
    "MRR2.append(search(['directed', 'San_Andreas_(film)'], ['Brad_Peyton', 'directed', 'San_Andreas_(film)']))\n",
    "MRR2.append(search(['wroteMusicFor', 'Cosmopolitan_(film)'], ['Andrew_Lockington', 'wroteMusicFor', 'Cosmopolitan_(film)']))\n",
    "MRR2.append(search(['Kiribati', 'hasCapital'], ['Kiribati', 'hasCapital', 'South_Tarawa']))\n",
    "MRR2.append(search(['Charles_the_Fat', 'East_Francia'], ['Charles_the_Fat', 'wasBornIn', 'East_Francia']))\n",
    "MRR2.append(search(['hasChild', 'Michelle_Obama'], ['Marian_Shields_Robinson', 'hasChild', 'Michelle_Obama']))\n",
    "MRR2.append(search(['Greenland', 'hasCurrency'], ['Greenland', 'hasCurrency', 'Danish_krone']))\n",
    "MRR2 = statistics.mean(MRR2)\n",
    "print(MRR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The benchmark questions are carefully selected** to have exactly one correct answer, so the MRR still being below 1.0 is unexpected. **Due to a mistake**, the query below does in fact have two correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Apart from the golden triple given as an argument in the function, there exists another correct answer:\n",
    "# 'Chris_Rael', 'wroteMusicFor', 'Cosmopolitan_(film)'\n",
    "print(search(['wroteMusicFor', 'Cosmopolitan_(film)'], ['Andrew_Lockington', 'wroteMusicFor', 'Cosmopolitan_(film)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This unintended result highlights a problem**: The search function returns one triple as the correct answer to the query, which is not ideal. **For now, the search function will be modified to return single entities** (instead of triples) and compare them with a golden answer that corresponds to one entity. The idea is that the usual query contains two entities whose connection to each other is a third entity which is then returned as the answer.\n",
    "\n",
    "In this case **the easiest way to proceed is to assume that the query won't contain predicates** - but only subjects and objects - so that the predicates are the 'edges' if the dataset is pictured as a knowledge graph. The following function looks for edges that are incident to the search terms and ranks them in importance to return the \"answer\" edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queryList, goldenAnswer): # Change goldenTriple to goldenAnswer\n",
    "    rpr = 0\n",
    "    mainIndexList = []\n",
    "    importance = []\n",
    "    predicateListFrequencies = []\n",
    "    for i in range(0, len(queryList)):\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Subject\"] == queryList[i]].tolist())\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Object\"] == queryList[i]].tolist())\n",
    "    mainIndexListUnique = list(dict.fromkeys(mainIndexList))\n",
    "    for m in mainIndexListUnique:\n",
    "        importance.append(100**mainIndexList.count(m)) # Ignore importance of predicates in query (change ranking)\n",
    "    df2 = df.iloc[mainIndexListUnique]\n",
    "    df2.loc[:, \"Relevance\"] = importance\n",
    "    predicateList = df2.loc[:, 'Predicate'].tolist()\n",
    "    predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "    for k in predicateList:\n",
    "        predicateListFrequencies.append(predicateList.count(k))\n",
    "    lowestUniquePredicateFrequency = min(predicateListFrequencies)\n",
    "    for n in predicateListUnique:\n",
    "        df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance'] = (df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance']/predicateList.count(n)) * lowestUniquePredicateFrequency\n",
    "    df2 = df2.sort_values(by=['Relevance'], ascending=False)\n",
    "    df2 = df2.loc[:, \"Predicate\"] # Make result table consist of predicates only\n",
    "    if len(df2) >= 10:\n",
    "        for p in range(0, len(df2)):\n",
    "            if df2.iloc[p] == goldenAnswer: # Compare with golden answer predicate\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    else:\n",
    "        for p in range(0, 10):\n",
    "            if df2.iloc[p] == goldenAnswer: # \"\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    return rpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code below contains new benchmark questions** which don't take into account the possibility of multiple correct answers for one question worsening the RPR, **to give a less biased picture** (benchmark questions with predicates as search terms were removed from the sample queries to fulfil the assumption the new function makes about queries). \n",
    "\n",
    "Questions that contain two _not adjacent but connected_ nodes (subjects/objects) are also included to better compare this search function with a later version. These questions are marked with comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MRR3.append(search(['Barack_Obama', 'Marian_Shields_Robinson'], 'Michelle_Obama')) # 'isMarriedTo' and 'hasChild'\n",
    "MRR3.append(search(['Sigmund_Freud', 'Kesswil'], 'Carl_Jung')) #Jung worked with Freud and was born in Kesswil\n",
    "MRR3.append(search(['Battle_of_Talas', 'Tajikistan'], 'Kyrgyzstan')) #Tajikistan is next to where the battle took place\n",
    "MRR3.append(search(['Ricochet_(TV_production_company)', 'Sahara'], 'Unbreakable_(TV_series)')) # etc.\n",
    "MRR3.append(search(['Rocky_Johnson', 'Dwayne_Johnson'], 'hasChild'))\n",
    "MRR3.append(search(['Rome', 'http://www.comune.roma.it/'], 'hasWebsite'))\n",
    "MRR3.append(search(['Charles_the_Fat', 'East_Francia'], 'wasBornIn'))\n",
    "MRR3.append(search(['Luxembourg', 'Luxembourg_City'], 'hasCapital'))\n",
    "MRR3.append(search(['Toby_Barrett', 'Long_Point,_Ontario'], 'isLeaderOf'))\n",
    "MRR3.append(search(['Metra', 'North_Central_Service'], 'owns'))\n",
    "MRR3.append(search(['Gordon_Ramsay', 'Culinary_Genius_(TV_series)'], 'created'))\n",
    "MRR3.append(search(['Kugelmugel', 'German_language'], 'hasOfficialLanguage'))\n",
    "MRR3.append(search(['Yoshitami_Kuroiwa', 'Godzilla_1985'], 'edited'))\n",
    "MRR3.append(search(['Gisborne_Airport', 'Auckland_Airport'], 'isConnectedTo'))\n",
    "MRR3.append(search(['Macedonia_(ancient_kingdom)', 'Siege_of_Cyropolis'], 'participatedIn'))\n",
    "MRR3.append(search(['Aristotle', 'Euboea'], 'diedIn'))\n",
    "MRR3.append(search(['Latvia', 'Belarus'], 'hasNeighbor'))\n",
    "MRR3.append(search(['Luigi_Ambrosio', 'Ennio_de_Giorgi'], 'hasAcademicAdvisor'))\n",
    "MRR3.append(search(['Jeff_Bezos', 'Amazon.com'], 'created'))\n",
    "MRR3.append(search(['Tatsuro_Yamashita', 'Ride_On_Time_(album)'], 'created'))\n",
    "MRR3 = statistics.mean(MRR3)\n",
    "print(MRR3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A score of 0.75 for **the MRR is very good** considering that out of 20 benchmark queries, the first 4 cannot have anything but 0 as their RPR.\n",
    "\n",
    "The good score is likely owing to the fact that the 'correct' answer doesn't have to be an entire triple anymore, it only needs to be the correct predicate. Moreover, **excluding predicates from the query makes it less likely for a question to have several correct answers**. The following example demonstrates this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(search(['Jeff_Bezos', 'Amazon.com'], 'created'))\n",
    "print(search(['Jeff_Bezos', 'Amazon.com'], 'owns'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This result is acceptable** because the \"created\" predicate is arguably a more important link between Jeff Bezos and Amazon than him owning a share of the company. This is in contrast to the \"wroteMusicFor\" example earlier in which a subject was favoured over the other without good reason. If the search function would be modified to treat not predicates, but either subjects or objects as the edges of the knowledge graph, the problem of several answer edges having the same importance would still persist and would have to be addressed.\n",
    "\n",
    "Putting this aside and assuming that the MRR of this function will go up to 0.8 with minor tweaking of the ranking algorithm, it is clear that **the main weakness of the new search function is finding the relationship between search terms that don't appear together in any triple** in the knowledge graph of the Yago dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfinished\n",
    "def search(queryList, goldenAnswer):\n",
    "    rpr = 0\n",
    "    edgeExists = False\n",
    "    mainIndexList = []\n",
    "    importance = []\n",
    "    predicateListFrequencies = []\n",
    "    for i in range(0, len(queryList)):\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Subject\"] == queryList[i]].tolist())\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Object\"] == queryList[i]].tolist())\n",
    "    mainIndexListUnique = list(dict.fromkeys(mainIndexList))\n",
    "    for m in mainIndexListUnique:\n",
    "        importance.append(mainIndexList.count(m)**10) # Minor change to ranking\n",
    "    if max(importance) == 1024: # Check if there is an edge between search terms\n",
    "        edgeExists = True\n",
    "    df2 = df.iloc[mainIndexListUnique]\n",
    "    df2.loc[:, \"Relevance\"] = importance\n",
    "    predicateList = df2.loc[:, 'Predicate'].tolist()\n",
    "    predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "    nodeListUnique = list(pd.unique(df2[['Subject', 'Object']].values.ravel('K'))) # Get all nodes in subgraph df2\n",
    "    for q in queryList:\n",
    "        nodeListUnique.remove(q) # Separate query terms from other nodes in subgraph df2\n",
    "    for k in predicateList:\n",
    "        predicateListFrequencies.append(predicateList.count(k))\n",
    "    lowestUniquePredicateFrequency = min(predicateListFrequencies)\n",
    "    if edgeExists: # Rank predicates as usual if search terms are adjacent\n",
    "        for n in predicateListUnique:\n",
    "            df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance'] = (df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance']/predicateList.count(n)) * lowestUniquePredicateFrequency\n",
    "    else: # Otherwise rank the nodes in the subgraph df2\n",
    "        for n in nodeListUnique:\n",
    "            numberOfEdgesToSearchTerms = len(df2.loc) ###########\n",
    "            df2.loc[df2.loc[:, 'Subject'] == n, 'Relevance'] = (df2.loc[df2.loc[:, 'Subject'] == n, 'Relevance']) * (len(predicateListUnique)/37) * (numberOfEdgesToSearchTerms/len(queryList))\n",
    "    df2 = df2.sort_values(by=['Relevance'], ascending=False)\n",
    "    df2 = df2.loc[:, \"Predicate\"]\n",
    "    if len(df2) >= 10:\n",
    "        for p in range(0, len(df2)):\n",
    "            if df2.iloc[p] == goldenAnswer:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    else:\n",
    "        for p in range(0, 10):\n",
    "            if df2.iloc[p] == goldenAnswer:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    return rpr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
