{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "pd.options.mode.chained_assignment = None\n",
    "allPredicates = ['isLeaderOf', 'owns', 'isCitizenOf', 'isLocatedIn', 'hasMusicalRole', 'hasOfficialLanguage', 'edited', 'isConnectedTo', 'actedIn', 'imports', 'participatedIn', 'wasBornIn', 'dealsWith', 'created', 'diedIn', 'isPoliticianOf', 'wroteMusicFor', 'hasNeighbor', 'isMarriedTo', 'hasChild', 'isInterestedIn', 'isAffiliatedTo', 'hasCurrency', 'exports', 'happenedIn', 'hasGender', 'playsFor', 'directed', 'worksAt', 'graduatedFrom', 'hasCapital', 'influences', 'hasWonPrize', 'hasWebsite', 'livesIn', 'hasAcademicAdvisor', 'isKnownFor']\n",
    "df = pd.read_csv('C:/Users/lucan/Documents/Uni/Courses/Bachelor Semester Project/BSP1/code/yagoFactsCleaned.csv')\n",
    "df.columns = ['Subject', 'Predicate', 'Object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is similar to the code in \"yagoFactsSearchEngine.py\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queryList, goldenTriple):\n",
    "    rpr = 0\n",
    "    mainIndexList = []\n",
    "    importance = []\n",
    "    predicateListFrequencies = []\n",
    "    for i in range(0, len(queryList)):\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Subject\"] == queryList[i]].tolist())\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Predicate\"] == queryList[i]].tolist())\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Object\"] == queryList[i]].tolist())\n",
    "    mainIndexListUnique = list(dict.fromkeys(mainIndexList))\n",
    "    for m in mainIndexListUnique:\n",
    "        importance.append(100**mainIndexList.count(m))\n",
    "    df2 = df.iloc[mainIndexListUnique]\n",
    "    df2.loc[:, \"Relevance\"] = importance\n",
    "    predicateList = df2.loc[:, 'Predicate'].tolist()\n",
    "    predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "    for k in predicateList:\n",
    "        predicateListFrequencies.append(predicateList.count(k))\n",
    "    lowestUniquePredicateFrequency = min(predicateListFrequencies)\n",
    "    for n in predicateListUnique:\n",
    "        df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance'] = (df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance']/predicateList.count(n)) * lowestUniquePredicateFrequency\n",
    "    df2 = df2.sort_values(by=['Relevance'], ascending=False)\n",
    "    if len(df2) >= 10:\n",
    "        for p in range(0, len(df2)):\n",
    "            if df2.iloc[p][\"Subject\"] == goldenTriple[0] and df2.iloc[p][\"Predicate\"] == goldenTriple[1] and df2.iloc[p][\"Object\"] == goldenTriple[2]:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    else:\n",
    "        for p in range(0, 10):\n",
    "            if df2.iloc[p][\"Subject\"] == goldenTriple[0] and df2.iloc[p][\"Predicate\"] == goldenTriple[1] and df2.iloc[p][\"Object\"] == goldenTriple[2]:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    return rpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35900429088611807\n",
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RR = []\n",
    "RR.append(search(['Rocky_Johnson', 'Dwayne_Johnson'], ['Rocky_Johnson', 'hasChild', 'Dwayne_Johnson']))\n",
    "RR.append(search(['Rocky_Johnson', 'hasChild'], ['Rocky_Johnson', 'hasChild', 'Dwayne_Johnson']))\n",
    "RR.append(search(['Roman_Empire', 'hasCurrency'], ['Roman_Empire', 'hasCurrency', 'Sestertius']))\n",
    "RR.append(search(['Rome', 'http://www.comune.roma.it/'], ['Rome', 'hasWebsite', 'http://www.comune.roma.it/']))\n",
    "RR.append(search(['directed', 'San_Andreas_(film)'], ['Brad_Peyton', 'directed', 'San_Andreas_(film)']))\n",
    "RR.append(search(['wroteMusicFor', 'Cosmopolitan_(film)'], ['Andrew_Lockington', 'wroteMusicFor', 'Cosmopolitan_(film)']))\n",
    "RR.append(search(['Kiribati', 'hasCapital'], ['Kiribati', 'hasCapital', 'South_Tarawa']))\n",
    "RR.append(search(['Charles_the_Fat', 'East_Francia'], ['Charles_the_Fat', 'wasBornIn', 'East_Francia']))\n",
    "RR.append(search(['hasChild', 'Michelle_Obama'], ['Marian_Shields_Robinson', 'hasChild', 'Michelle_Obama']))\n",
    "RR.append(search(['Greenland', 'hasCurrency'], ['Greenland', 'hasCurrency', 'Danish_krone']))\n",
    "print(statistics.mean(RR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MRR is worse than expected and the search function takes a lot of time. This is largely due to the inclusion of triples that only have a matching predicate in the results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queryList, goldenTriple):\n",
    "    rpr = 0\n",
    "    mainIndexList = []\n",
    "    importance = []\n",
    "    predicateListFrequencies = []\n",
    "    for i in range(0, len(queryList)):\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Subject\"] == queryList[i]].tolist()) # Omit search for\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Object\"] == queryList[i]].tolist()) # rows with matching predicate\n",
    "    mainIndexListUnique = list(dict.fromkeys(mainIndexList))\n",
    "    for m in mainIndexListUnique:\n",
    "        if df.iloc[m][\"Predicate\"] in queryList:\n",
    "            importance.append(100**mainIndexList.count(m)*10) # Add value to rows with matching predicates\n",
    "        else:\n",
    "            importance.append(100**mainIndexList.count(m))\n",
    "    df2 = df.iloc[mainIndexListUnique]\n",
    "    df2.loc[:, \"Relevance\"] = importance\n",
    "    predicateList = df2.loc[:, 'Predicate'].tolist()\n",
    "    predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "    for k in predicateList:\n",
    "        predicateListFrequencies.append(predicateList.count(k))\n",
    "    lowestUniquePredicateFrequency = min(predicateListFrequencies)\n",
    "    for n in predicateListUnique:\n",
    "        df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance'] = (df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance']/predicateList.count(n)) * lowestUniquePredicateFrequency\n",
    "    df2 = df2.sort_values(by=['Relevance'], ascending=False)\n",
    "    if len(df2) >= 10:\n",
    "        for p in range(0, len(df2)):\n",
    "            if df2.iloc[p][\"Subject\"] == goldenTriple[0] and df2.iloc[p][\"Predicate\"] == goldenTriple[1] and df2.iloc[p][\"Object\"] == goldenTriple[2]:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    else:\n",
    "        for p in range(0, 10):\n",
    "            if df2.iloc[p][\"Subject\"] == goldenTriple[0] and df2.iloc[p][\"Predicate\"] == goldenTriple[1] and df2.iloc[p][\"Object\"] == goldenTriple[2]:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    return rpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RR = []\n",
    "RR.append(search(['Rocky_Johnson', 'Dwayne_Johnson'], ['Rocky_Johnson', 'hasChild', 'Dwayne_Johnson']))\n",
    "RR.append(search(['Rocky_Johnson', 'hasChild'], ['Rocky_Johnson', 'hasChild', 'Dwayne_Johnson']))\n",
    "RR.append(search(['Roman_Empire', 'hasCurrency'], ['Roman_Empire', 'hasCurrency', 'Sestertius']))\n",
    "RR.append(search(['Rome', 'http://www.comune.roma.it/'], ['Rome', 'hasWebsite', 'http://www.comune.roma.it/']))\n",
    "RR.append(search(['directed', 'San_Andreas_(film)'], ['Brad_Peyton', 'directed', 'San_Andreas_(film)']))\n",
    "RR.append(search(['wroteMusicFor', 'Cosmopolitan_(film)'], ['Andrew_Lockington', 'wroteMusicFor', 'Cosmopolitan_(film)']))\n",
    "RR.append(search(['Kiribati', 'hasCapital'], ['Kiribati', 'hasCapital', 'South_Tarawa']))\n",
    "RR.append(search(['Charles_the_Fat', 'East_Francia'], ['Charles_the_Fat', 'wasBornIn', 'East_Francia']))\n",
    "RR.append(search(['hasChild', 'Michelle_Obama'], ['Marian_Shields_Robinson', 'hasChild', 'Michelle_Obama']))\n",
    "RR.append(search(['Greenland', 'hasCurrency'], ['Greenland', 'hasCurrency', 'Danish_krone']))\n",
    "print(statistics.mean(RR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the benchmark queries has an RR of 0.5 despite the golden triple having the highest relevance score. This is a limitation of the benchmarking algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(search(['wroteMusicFor', 'Cosmopolitan_(film)'], ['Andrew_Lockington', 'wroteMusicFor', 'Cosmopolitan_(film)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function has a different ranking algorithm. It assumes that the query contains only subjects and objects (nodes). It only returns the rarest predicate (edge) that connects two nodes in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queryList, goldenAnswer): # Change goldenTriple to goldenAnswer\n",
    "    rpr = 0\n",
    "    mainIndexList = []\n",
    "    importance = []\n",
    "    predicateListFrequencies = []\n",
    "    for i in range(0, len(queryList)):\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Subject\"] == queryList[i]].tolist())\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Object\"] == queryList[i]].tolist())\n",
    "    mainIndexListUnique = list(dict.fromkeys(mainIndexList))\n",
    "    for m in mainIndexListUnique:\n",
    "        importance.append(100**(mainIndexList.count(m))) # Ignore predicates in query (change ranking)\n",
    "    df2 = df.iloc[mainIndexListUnique]\n",
    "    df2.loc[:, \"Relevance\"] = importance\n",
    "    predicateList = df2.loc[:, 'Predicate'].tolist()\n",
    "    predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "    for k in predicateList:\n",
    "        predicateListFrequencies.append(predicateList.count(k))\n",
    "    lowestUniquePredicateFrequency = min(predicateListFrequencies)\n",
    "    for n in predicateListUnique:\n",
    "        df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance'] = (df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance']/predicateList.count(n)) * lowestUniquePredicateFrequency\n",
    "    df2 = df2.sort_values(by=['Relevance'], ascending=False)\n",
    "    df2 = df2.loc[:, \"Predicate\"] # Make result table consist of predicates only\n",
    "    if len(df2) >= 10:\n",
    "        for p in range(0, len(df2)):\n",
    "            if df2.iloc[p] == goldenAnswer: # Compare with golden answer predicate\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    else:\n",
    "        for p in range(0, 10):\n",
    "            if df2.iloc[p] == goldenAnswer: # \"\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    return rpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "Wall time: 44.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Changed benchmark queries\n",
    "RR = []\n",
    "RR.append(search(['Barack_Obama', 'Marian_Shields_Robinson'], 'hasChildisMarriedTo'))\n",
    "RR.append(search(['Carl_Jung', 'Arbon_District'], 'wasBornInisLocatedIn'))\n",
    "RR.append(search(['Battle_of_Talas', 'Tajikistan'], 'happenedInhasNeighbor'))\n",
    "RR.append(search(['Ricochet_(TV_production_company)', 'Sahara'], 'createdisLocatedIn'))\n",
    "RR.append(search(['Rocky_Johnson', 'Dwayne_Johnson'], 'hasChild'))\n",
    "RR.append(search(['Rome', 'http://www.comune.roma.it/'], 'hasWebsite'))\n",
    "RR.append(search(['Charles_the_Fat', 'East_Francia'], 'wasBornIn'))\n",
    "RR.append(search(['Luxembourg', 'Luxembourg_City'], 'hasCapital'))\n",
    "RR.append(search(['Toby_Barrett', 'Long_Point,_Ontario'], 'isLeaderOf'))\n",
    "RR.append(search(['Metra', 'North_Central_Service'], 'owns'))\n",
    "RR.append(search(['Gordon_Ramsay', 'Culinary_Genius_(TV_series)'], 'created'))\n",
    "RR.append(search(['Kugelmugel', 'German_language'], 'hasOfficialLanguage'))\n",
    "RR.append(search(['Yoshitami_Kuroiwa', 'Godzilla_1985'], 'edited'))\n",
    "RR.append(search(['Gisborne_Airport', 'Auckland_Airport'], 'isConnectedTo'))\n",
    "RR.append(search(['Macedonia_(ancient_kingdom)', 'Siege_of_Cyropolis'], 'participatedIn'))\n",
    "RR.append(search(['Aristotle', 'Euboea'], 'diedIn'))\n",
    "RR.append(search(['Latvia', 'Belarus'], 'hasNeighbor'))\n",
    "RR.append(search(['Luigi_Ambrosio', 'Ennio_de_Giorgi'], 'hasAcademicAdvisor'))\n",
    "RR.append(search(['Jeff_Bezos', 'Amazon.com'], 'created'))\n",
    "RR.append(search(['Tatsuro_Yamashita', 'Ride_On_Time_(album)'], 'created'))\n",
    "print(statistics.mean(RR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two of the benchmark queries have a score of 0.5 which is due to the ranking algorithm in the first case and due to a mistake regarding the golden answer in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(search(['Gisborne_Airport', 'Auckland_Airport'], 'isConnectedTo'))\n",
    "print(search(['Macedonia_(ancient_kingdom)', 'Siege_of_Cyropolis'], 'participatedIn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is the same as above, with slightly different ranking, and it is benchmarked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queryList, goldenAnswer):\n",
    "    rpr = 0\n",
    "    mainIndexList = []\n",
    "    importance = []\n",
    "    predicateListFrequencies = []\n",
    "    for i in range(0, len(queryList)):\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Subject\"] == queryList[i]].tolist())\n",
    "        mainIndexList.extend(df.index[df.loc[:, \"Object\"] == queryList[i]].tolist())\n",
    "    mainIndexListUnique = list(dict.fromkeys(mainIndexList))\n",
    "    for m in mainIndexListUnique:\n",
    "        importance.append(100**(mainIndexList.count(m)*2)) # Fine-tune ranking\n",
    "    df2 = df.iloc[mainIndexListUnique]\n",
    "    df2.loc[:, \"Relevance\"] = importance\n",
    "    predicateList = df2.loc[:, 'Predicate'].tolist()\n",
    "    predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "    for k in predicateList:\n",
    "        predicateListFrequencies.append(predicateList.count(k))\n",
    "    lowestUniquePredicateFrequency = min(predicateListFrequencies)\n",
    "    for n in predicateListUnique:\n",
    "        df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance'] = (df2.loc[df2.loc[:, 'Predicate'] == n, 'Relevance']/predicateList.count(n)) * lowestUniquePredicateFrequency\n",
    "    df2 = df2.sort_values(by=['Relevance'], ascending=False)\n",
    "    df2 = df2.loc[:, \"Predicate\"]\n",
    "    if len(df2) >= 10:\n",
    "        for p in range(0, len(df2)):\n",
    "            if df2.iloc[p] == goldenAnswer:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    else:\n",
    "        for p in range(0, 10):\n",
    "            if df2.iloc[p] == goldenAnswer:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    return rpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "Wall time: 44.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RR = []\n",
    "RR.append(search(['Barack_Obama', 'Marian_Shields_Robinson'], 'hasChildisMarriedTo'))\n",
    "RR.append(search(['Carl_Jung', 'Arbon_District'], 'wasBornInisLocatedIn'))\n",
    "RR.append(search(['Battle_of_Talas', 'Tajikistan'], 'happenedInhasNeighbor'))\n",
    "RR.append(search(['Ricochet_(TV_production_company)', 'Sahara'], 'createdisLocatedIn'))\n",
    "RR.append(search(['Rocky_Johnson', 'Dwayne_Johnson'], 'hasChild'))\n",
    "RR.append(search(['Rome', 'http://www.comune.roma.it/'], 'hasWebsite'))\n",
    "RR.append(search(['Charles_the_Fat', 'East_Francia'], 'wasBornIn'))\n",
    "RR.append(search(['Luxembourg', 'Luxembourg_City'], 'hasCapital'))\n",
    "RR.append(search(['Toby_Barrett', 'Long_Point,_Ontario'], 'isLeaderOf'))\n",
    "RR.append(search(['Metra', 'North_Central_Service'], 'owns'))\n",
    "RR.append(search(['Gordon_Ramsay', 'Culinary_Genius_(TV_series)'], 'created'))\n",
    "RR.append(search(['Kugelmugel', 'German_language'], 'hasOfficialLanguage'))\n",
    "RR.append(search(['Yoshitami_Kuroiwa', 'Godzilla_1985'], 'edited'))\n",
    "RR.append(search(['Gisborne_Airport', 'Auckland_Airport'], 'isConnectedTo'))\n",
    "RR.append(search(['Macedonia_(ancient_kingdom)', 'Siege_of_Cyropolis'], 'happenedIn')) # Changed golden answer\n",
    "RR.append(search(['Aristotle', 'Euboea'], 'diedIn'))\n",
    "RR.append(search(['Latvia', 'Belarus'], 'hasNeighbor'))\n",
    "RR.append(search(['Luigi_Ambrosio', 'Ennio_de_Giorgi'], 'hasAcademicAdvisor'))\n",
    "RR.append(search(['Jeff_Bezos', 'Amazon.com'], 'created'))\n",
    "RR.append(search(['Tatsuro_Yamashita', 'Ride_On_Time_(album)'], 'created'))\n",
    "print(statistics.mean(RR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function below, if there is no edge incident to two nodes in the query, the function will return the rarest length-2 path that connects two nodes in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queryList, goldenAnswer):\n",
    "    rpr = 0\n",
    "    df2 = df.loc[df['Subject'].isin(queryList) | df['Object'].isin(queryList)]\n",
    "    tf = df2.merge(right=df2, left_on='Object', right_on='Subject')\n",
    "    tf['Subject'], tf['Predicate'], tf['Object'] = tf['Subject_x'], tf['Predicate_x']+tf['Predicate_y'], tf['Object_y']\n",
    "    tf = tf.loc[:, ['Subject', 'Predicate', 'Object']]\n",
    "    df2 = pd.concat([df2, tf])\n",
    "    predicateList = df2['Predicate'].tolist()\n",
    "    predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "    df2['Relevance'] = 0\n",
    "    df2.loc[df2['Subject'].isin(queryList), 'Relevance'] += 1\n",
    "    df2.loc[df2['Object'].isin(queryList), 'Relevance'] += 1\n",
    "    df2.loc[df2['Predicate'].isin(allPredicates), 'Relevance'] += 0.1\n",
    "    for n in predicateListUnique:\n",
    "        df2.loc[df2['Predicate'] == n, 'Relevance'] += 1/(predicateList.count(n)*10)\n",
    "    df2.sort_values(by=['Relevance'], ascending=False, inplace=True)\n",
    "    for p in range(0, len(df2['Predicate'].head())):\n",
    "        if df2['Predicate'].head().iloc[p] == goldenAnswer:\n",
    "            rpr = 1/(p+1)\n",
    "            break\n",
    "    return rpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RR = []\n",
    "RR.append(search(['Barack_Obama', 'Marian_Shields_Robinson'], 'hasChildisMarriedTo'))\n",
    "RR.append(search(['Sigmund_Freud', 'Kesswil'], 'hasAcademicAdvisorwasBornIn'))\n",
    "RR.append(search(['Battle_of_Talas', 'Tajikistan'], 'happenedInhasNeighbor'))\n",
    "RR.append(search(['Ricochet_(TV_production_company)', 'Sahara'], 'createdisLocatedIn'))\n",
    "RR.append(search(['Rocky_Johnson', 'Dwayne_Johnson'], 'hasChild'))\n",
    "RR.append(search(['Rome', 'http://www.comune.roma.it/'], 'hasWebsite'))\n",
    "RR.append(search(['Charles_the_Fat', 'East_Francia'], 'wasBornIn'))\n",
    "RR.append(search(['Luxembourg', 'Luxembourg_City'], 'hasCapital'))\n",
    "RR.append(search(['Toby_Barrett', 'Long_Point,_Ontario'], 'isLeaderOf'))\n",
    "RR.append(search(['Metra', 'North_Central_Service'], 'owns'))\n",
    "RR.append(search(['Gordon_Ramsay', 'Culinary_Genius_(TV_series)'], 'created'))\n",
    "RR.append(search(['Kugelmugel', 'German_language'], 'hasOfficialLanguage'))\n",
    "RR.append(search(['Yoshitami_Kuroiwa', 'Godzilla_1985'], 'edited'))\n",
    "RR.append(search(['Gisborne_Airport', 'Auckland_Airport'], 'isConnectedTo'))\n",
    "RR.append(search(['Macedonia_(ancient_kingdom)', 'Siege_of_Cyropolis'], 'happenedIn'))\n",
    "RR.append(search(['Aristotle', 'Euboea'], 'diedIn'))\n",
    "RR.append(search(['Latvia', 'Belarus'], 'hasNeighbor'))\n",
    "RR.append(search(['Luigi_Ambrosio', 'Ennio_de_Giorgi'], 'hasAcademicAdvisor'))\n",
    "RR.append(search(['Jeff_Bezos', 'Amazon.com'], 'created'))\n",
    "RR.append(search(['Tatsuro_Yamashita', 'Ride_On_Time_(album)'], 'created'))\n",
    "print(statistics.mean(RR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RR in the following benchmark queries can be improved by ignoring cyclic paths in the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(search(['Barack_Obama', 'Marian_Shields_Robinson'], 'hasChildisMarriedTo'))\n",
    "print(search(['Battle_of_Talas', 'Tajikistan'], 'happenedInhasNeighbor'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the query below however, there is no length-2 path between the nodes as far as this function is concerned, despite of the nodes being 2-hop connected via the node 'Carl_Jung'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(search(['Sigmund_Freud', 'Kesswil'], 'hasAcademicAdvisorwasBornIn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function ignores cyclic paths by giving them a relevance score of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queryList, goldenAnswer):\n",
    "    rpr = 0\n",
    "    df2 = df.loc[df['Subject'].isin(queryList) | df['Object'].isin(queryList)]\n",
    "    tf = df2.merge(right=df2, left_on='Object', right_on='Subject')\n",
    "    tf['Subject'], tf['Predicate'], tf['Object'] = tf['Subject_x'], tf['Predicate_x']+tf['Predicate_y'], tf['Object_y']\n",
    "    tf = tf.loc[:, ['Subject', 'Predicate', 'Object']]\n",
    "    df2 = pd.concat([df2, tf])\n",
    "    predicateList = df2['Predicate'].tolist()\n",
    "    predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "    df2['Relevance'] = 0\n",
    "    df2.loc[df2['Subject'].isin(queryList), 'Relevance'] += 1\n",
    "    df2.loc[df2['Object'].isin(queryList), 'Relevance'] += 1\n",
    "    df2.loc[df2['Predicate'].isin(allPredicates), 'Relevance'] += 0.1\n",
    "    df2.loc[df2['Subject'] == df2['Object'], 'Relevance'] = 0 # Ignore cyclic paths\n",
    "    for n in predicateListUnique:\n",
    "        df2.loc[df2['Predicate'] == n, 'Relevance'] += 1/(predicateList.count(n)*10)\n",
    "    df2.sort_values(by=['Relevance'], ascending=False, inplace=True)\n",
    "    for p in range(0, len(df2['Predicate'].head())):\n",
    "        if df2['Predicate'].head().iloc[p] == goldenAnswer:\n",
    "            rpr = 1/(p+1)\n",
    "            break\n",
    "    return rpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RR = []\n",
    "RR.append(search(['Barack_Obama', 'Marian_Shields_Robinson'], 'hasChildisMarriedTo'))\n",
    "RR.append(search(['Sigmund_Freud', 'Kesswil'], 'hasAcademicAdvisorwasBornIn'))\n",
    "RR.append(search(['Battle_of_Talas', 'Tajikistan'], 'happenedInhasNeighbor'))\n",
    "RR.append(search(['Ricochet_(TV_production_company)', 'Sahara'], 'createdisLocatedIn'))\n",
    "RR.append(search(['Rocky_Johnson', 'Dwayne_Johnson'], 'hasChild'))\n",
    "RR.append(search(['Rome', 'http://www.comune.roma.it/'], 'hasWebsite'))\n",
    "RR.append(search(['Charles_the_Fat', 'East_Francia'], 'wasBornIn'))\n",
    "RR.append(search(['Luxembourg', 'Luxembourg_City'], 'hasCapital'))\n",
    "RR.append(search(['Toby_Barrett', 'Long_Point,_Ontario'], 'isLeaderOf'))\n",
    "RR.append(search(['Metra', 'North_Central_Service'], 'owns'))\n",
    "RR.append(search(['Gordon_Ramsay', 'Culinary_Genius_(TV_series)'], 'created'))\n",
    "RR.append(search(['Kugelmugel', 'German_language'], 'hasOfficialLanguage'))\n",
    "RR.append(search(['Yoshitami_Kuroiwa', 'Godzilla_1985'], 'edited'))\n",
    "RR.append(search(['Gisborne_Airport', 'Auckland_Airport'], 'isConnectedTo'))\n",
    "RR.append(search(['Macedonia_(ancient_kingdom)', 'Siege_of_Cyropolis'], 'happenedIn'))\n",
    "RR.append(search(['Aristotle', 'Euboea'], 'diedIn'))\n",
    "RR.append(search(['Latvia', 'Belarus'], 'hasNeighbor'))\n",
    "RR.append(search(['Luigi_Ambrosio', 'Ennio_de_Giorgi'], 'hasAcademicAdvisor'))\n",
    "RR.append(search(['Jeff_Bezos', 'Amazon.com'], 'created'))\n",
    "RR.append(search(['Tatsuro_Yamashita', 'Ride_On_Time_(album)'], 'created'))\n",
    "print(statistics.mean(RR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
