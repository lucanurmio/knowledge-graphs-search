{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "MRR1 = []\n",
    "MRR2 = []\n",
    "MRR3 = []\n",
    "pd.options.mode.chained_assignment = None\n",
    "df = pd.read_csv('C:/Users/lucan/Documents/Uni/Courses/Bachelor Semester Project/BSP1/code/yagoFactsCleaned.csv')\n",
    "df.columns = ['Subject', 'Predicate', 'Object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is similar to the code in \"yagoFactsSearchEngine.py\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queryList, goldenTriple):\n",
    "    rpr = 0\n",
    "    for l in queryList:\n",
    "        mainIndexList = []\n",
    "        importance = []\n",
    "        predicateListFrequencies = []\n",
    "        for i in range(0, len(queryList)):\n",
    "            mainIndexList.extend(df.index[df.loc[:, \"Subject\"] == queryList[i]].tolist())\n",
    "            mainIndexList.extend(df.index[df.loc[:, \"Predicate\"] == queryList[i]].tolist())\n",
    "            mainIndexList.extend(df.index[df.loc[:, \"Object\"] == queryList[i]].tolist())\n",
    "        mainIndexListUnique = list(dict.fromkeys(mainIndexList))\n",
    "        for m in mainIndexListUnique:\n",
    "            importance.append(100**mainIndexList.count(m))\n",
    "        df2 = df.iloc[mainIndexListUnique]\n",
    "        df2.loc[:, \"Relevance\"] = importance\n",
    "        predicateList = df2.loc[:, 'Predicate'].tolist()\n",
    "        predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "        for k in predicateList:\n",
    "            predicateListFrequencies.append(predicateList.count(k))\n",
    "        lowestUniquePredicateFrequency = min(predicateListFrequencies)\n",
    "        for n in predicateListUnique:\n",
    "            df2.loc[df.loc[:, 'Predicate'] == n, 'Relevance'] = (df2.loc[df.loc[:, 'Predicate'] == n, 'Relevance']/predicateList.count(n)) * lowestUniquePredicateFrequency\n",
    "    df2 = df2.sort_values(by=['Relevance'], ascending=False)\n",
    "    if len(df2) >= 10:\n",
    "        for p in range(0, len(df2)):\n",
    "            if df2.iloc[p][\"Subject\"] == goldenTriple[0] and df2.iloc[p][\"Predicate\"] == goldenTriple[1] and df2.iloc[p][\"Object\"] == goldenTriple[2]:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    else:\n",
    "        for p in range(0, 10):\n",
    "            if df2.iloc[p][\"Subject\"] == goldenTriple[0] and df2.iloc[p][\"Predicate\"] == goldenTriple[1] and df2.iloc[p][\"Object\"] == goldenTriple[2]:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    return rpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean reciprocal rank (MRR) of this search function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35900429088611807\n",
      "Wall time: 8min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MRR1.append(search(['Rocky_Johnson', 'Dwayne_Johnson'], ['Rocky_Johnson', 'hasChild', 'Dwayne_Johnson']))\n",
    "MRR1.append(search(['Rocky_Johnson', 'hasChild'], ['Rocky_Johnson', 'hasChild', 'Dwayne_Johnson']))\n",
    "MRR1.append(search(['Roman_Empire', 'hasCurrency'], ['Roman_Empire', 'hasCurrency', 'Sestertius']))\n",
    "MRR1.append(search(['Rome', 'http://www.comune.roma.it/'], ['Rome', 'hasWebsite', 'http://www.comune.roma.it/']))\n",
    "MRR1.append(search(['directed', 'San_Andreas_(film)'], ['Brad_Peyton', 'directed', 'San_Andreas_(film)']))\n",
    "MRR1.append(search(['wroteMusicFor', 'Cosmopolitan_(film)'], ['Andrew_Lockington', 'wroteMusicFor', 'Cosmopolitan_(film)']))\n",
    "MRR1.append(search(['Kiribati', 'hasCapital'], ['Kiribati', 'hasCapital', 'South_Tarawa']))\n",
    "MRR1.append(search(['Charles_the_Fat', 'East_Francia'], ['Charles_the_Fat', 'wasBornIn', 'East_Francia']))\n",
    "MRR1.append(search(['hasChild', 'Michelle_Obama'], ['Marian_Shields_Robinson', 'hasChild', 'Michelle_Obama']))\n",
    "MRR1.append(search(['Greenland', 'hasCurrency'], ['Greenland', 'hasCurrency', 'Danish_krone']))\n",
    "MRR1 = statistics.mean(MRR1)\n",
    "print(MRR1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MRR is **worse than expected** and the search function takes a lot of time. **Fixing the issues requires changing the ranking algorithm** to pay more attention to predicates in selected rows, instead of letting the function select all rows with a matching predicate and ranking those. The main changes in the following function are marked with comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queryList, goldenTriple):\n",
    "    rpr = 0\n",
    "    for l in queryList:\n",
    "        mainIndexList = []\n",
    "        importance = []\n",
    "        predicateListFrequencies = []\n",
    "        for i in range(0, len(queryList)):\n",
    "            mainIndexList.extend(df.index[df.loc[:, \"Subject\"] == queryList[i]].tolist()) # Omitting the search for\n",
    "            mainIndexList.extend(df.index[df.loc[:, \"Object\"] == queryList[i]].tolist()) # rows with matching predicate\n",
    "        mainIndexListUnique = list(dict.fromkeys(mainIndexList))\n",
    "        for m in mainIndexListUnique:\n",
    "            if df.iloc[m][\"Predicate\"] in queryList:\n",
    "                importance.append(100**mainIndexList.count(m)*10) # Adding value to rows with matching predicates\n",
    "            else:\n",
    "                importance.append(100**mainIndexList.count(m))\n",
    "        df2 = df.iloc[mainIndexListUnique]\n",
    "        df2.loc[:, \"Relevance\"] = importance\n",
    "        predicateList = df2.loc[:, 'Predicate'].tolist()\n",
    "        predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "        for k in predicateList:\n",
    "            predicateListFrequencies.append(predicateList.count(k))\n",
    "        lowestUniquePredicateFrequency = min(predicateListFrequencies)\n",
    "        for n in predicateListUnique:\n",
    "            df2.loc[df.loc[:, 'Predicate'] == n, 'Relevance'] = (df2.loc[df.loc[:, 'Predicate'] == n, 'Relevance']/predicateList.count(n)) * lowestUniquePredicateFrequency\n",
    "    df2 = df2.sort_values(by=['Relevance'], ascending=False)\n",
    "    if len(df2) >= 10:\n",
    "        for p in range(0, len(df2)):\n",
    "            if df2.iloc[p][\"Subject\"] == goldenTriple[0] and df2.iloc[p][\"Predicate\"] == goldenTriple[1] and df2.iloc[p][\"Object\"] == goldenTriple[2]:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    else:\n",
    "        for p in range(0, 10):\n",
    "            if df2.iloc[p][\"Subject\"] == goldenTriple[0] and df2.iloc[p][\"Predicate\"] == goldenTriple[1] and df2.iloc[p][\"Object\"] == goldenTriple[2]:\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    return rpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MRR of this function with the exact same benchmark queries is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MRR2.append(search(['Rocky_Johnson', 'Dwayne_Johnson'], ['Rocky_Johnson', 'hasChild', 'Dwayne_Johnson']))\n",
    "MRR2.append(search(['Rocky_Johnson', 'hasChild'], ['Rocky_Johnson', 'hasChild', 'Dwayne_Johnson']))\n",
    "MRR2.append(search(['Roman_Empire', 'hasCurrency'], ['Roman_Empire', 'hasCurrency', 'Sestertius']))\n",
    "MRR2.append(search(['Rome', 'http://www.comune.roma.it/'], ['Rome', 'hasWebsite', 'http://www.comune.roma.it/']))\n",
    "MRR2.append(search(['directed', 'San_Andreas_(film)'], ['Brad_Peyton', 'directed', 'San_Andreas_(film)']))\n",
    "MRR2.append(search(['wroteMusicFor', 'Cosmopolitan_(film)'], ['Andrew_Lockington', 'wroteMusicFor', 'Cosmopolitan_(film)']))\n",
    "MRR2.append(search(['Kiribati', 'hasCapital'], ['Kiribati', 'hasCapital', 'South_Tarawa']))\n",
    "MRR2.append(search(['Charles_the_Fat', 'East_Francia'], ['Charles_the_Fat', 'wasBornIn', 'East_Francia']))\n",
    "MRR2.append(search(['hasChild', 'Michelle_Obama'], ['Marian_Shields_Robinson', 'hasChild', 'Michelle_Obama']))\n",
    "MRR2.append(search(['Greenland', 'hasCurrency'], ['Greenland', 'hasCurrency', 'Danish_krone']))\n",
    "MRR2 = statistics.mean(MRR2)\n",
    "print(MRR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The benchmark questions are carefully selected** to have exactly one correct answer, so the MRR still being below 1.0 is unexpected. **Due to a mistake**, the query below does in fact have two correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Apart from the golden triple given as an argument in the function, there exists another correct answer:\n",
    "# 'Chris_Rael', 'wroteMusicFor', 'Cosmopolitan_(film)'\n",
    "print(search(['wroteMusicFor', 'Cosmopolitan_(film)'], ['Andrew_Lockington', 'wroteMusicFor', 'Cosmopolitan_(film)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This unintended result highlights a problem: **The search function returns one triple as the correct answer to the query, which is not ideal. For now, the search function will be modified to return single entities** (instead of triples) and compare them with a golden answer that corresponds to one entity. The idea is that the usual query contains two entities whose connection to each other is a third entity which is then returned as the answer.\n",
    "\n",
    "In this case **the easiest way to proceed is to assume that the query won't contain predicates** - but only subjects and objects - so that the predicates are the 'edges' if the dataset is pictured as a knowledge graph. The following function looks for edges that are incident to the search terms and ranks them in importance to return the \"answer\" edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queryList, goldenAnswer): # Change goldenTriple to goldenAnswer\n",
    "    rpr = 0\n",
    "    for l in queryList:\n",
    "        mainIndexList = []\n",
    "        importance = []\n",
    "        predicateListFrequencies = []\n",
    "        for i in range(0, len(queryList)):\n",
    "            mainIndexList.extend(df.index[df.loc[:, \"Subject\"] == queryList[i]].tolist())\n",
    "            mainIndexList.extend(df.index[df.loc[:, \"Object\"] == queryList[i]].tolist())\n",
    "        mainIndexListUnique = list(dict.fromkeys(mainIndexList))\n",
    "        for m in mainIndexListUnique:\n",
    "            importance.append(100**mainIndexList.count(m)) # Change this part to ignore predicates in the query\n",
    "        df2 = df.iloc[mainIndexListUnique]\n",
    "        df2.loc[:, \"Relevance\"] = importance\n",
    "        predicateList = df2.loc[:, 'Predicate'].tolist()\n",
    "        predicateListUnique = list(dict.fromkeys(predicateList))\n",
    "        for k in predicateList:\n",
    "            predicateListFrequencies.append(predicateList.count(k))\n",
    "        lowestUniquePredicateFrequency = min(predicateListFrequencies)\n",
    "        for n in predicateListUnique:\n",
    "            df2.loc[df.loc[:, 'Predicate'] == n, 'Relevance'] = (df2.loc[df.loc[:, 'Predicate'] == n, 'Relevance']/predicateList.count(n)) * lowestUniquePredicateFrequency\n",
    "    df2 = df2.sort_values(by=['Relevance'], ascending=False)\n",
    "    df2 = df2.loc[:, \"Predicate\"] # Make result table consist of predicates only\n",
    "    if len(df2) >= 10:\n",
    "        for p in range(0, len(df2)):\n",
    "            if df2.iloc[p] == goldenAnswer[0]: # Compare with golden answer predicate\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    else:\n",
    "        for p in range(0, 10):\n",
    "            if df2.iloc[p] == goldenAnswer[0]: # \"\n",
    "                rpr = 1/(p+1)\n",
    "                break\n",
    "    return rpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code below contains new benchmark questions** which don't take into account the possibility of multiple correct answers for one question worsening the RPR, **to give a less biased picture** (benchmark questions with predicates as search terms were removed from the sample queries to fulfil the assumption the new function makes about queries). \n",
    "\n",
    "Questions that contain two _not adjacent but connected_ nodes (subjects/objects) are also included to better compare this search function with later versions. The plan is that, at some point, the function will return several nodes and edges that represent the relationship between the search terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "Wall time: 7min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MRR3.append(search(['Barack_Obama', 'Marian_Shields_Robinson'], ['Michelle_Obama', 'hasChild', 'isMarriedTo']))\n",
    "MRR3.append(search(['Sigmund_Freud', 'Kesswil'], ['Carl_Jung', 'hasAcademicAdvisor', 'wasBornIn']))\n",
    "MRR3.append(search(['Battle_of_Talas', 'Tajikistan'], ['Kyrgyzstan', 'hasNeighbor', 'happenedIn']))\n",
    "MRR3.append(search(['Ricochet_(TV_production_company)', 'Sahara'], ['Unbreakable_(TV_series)', 'created', 'isLocatedIn']))\n",
    "MRR3.append(search(['Rocky_Johnson', 'Dwayne_Johnson'], ['hasChild']))\n",
    "MRR3.append(search(['Rome', 'http://www.comune.roma.it/'], ['hasWebsite']))\n",
    "MRR3.append(search(['Charles_the_Fat', 'East_Francia'], ['wasBornIn']))\n",
    "MRR3.append(search(['Luxembourg', 'Luxembourg_City'], ['hasCapital']))\n",
    "MRR3.append(search(['Toby_Barrett', 'Long_Point,_Ontario'], ['isLeaderOf']))\n",
    "MRR3.append(search(['Metra', 'North_Central_Service'], ['owns']))\n",
    "MRR3.append(search(['Gordon_Ramsay', 'Culinary_Genius_(TV_series)'], ['created']))\n",
    "MRR3.append(search(['Kugelmugel', 'German_language'], ['hasOfficialLanguage']))\n",
    "MRR3.append(search(['Yoshitami_Kuroiwa', 'Godzilla_1985'], ['edited']))\n",
    "MRR3.append(search(['Gisborne_Airport', 'Auckland_Airport'], ['isConnectedTo']))\n",
    "MRR3.append(search(['Macedonia_(ancient_kingdom)', 'Siege_of_Cyropolis'], ['participatedIn']))\n",
    "MRR3.append(search(['Aristotle', 'Euboea'], ['diedIn']))\n",
    "MRR3.append(search(['Latvia', 'Belarus'], ['hasNeighbor']))\n",
    "MRR3.append(search(['Luigi_Ambrosio', 'Ennio_de_Giorgi'], ['hasAcademicAdvisor']))\n",
    "MRR3.append(search(['Jeff_Bezos', 'Amazon.com'], ['created']))\n",
    "MRR3.append(search(['Tatsuro_Yamashita', 'Ride_On_Time_(album)'], ['created']))\n",
    "MRR3 = statistics.mean(MRR3)\n",
    "print(MRR3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A score of 0.75 for **the MRR is very good** considering that out of 20 benchmark queries, the first 4 cannot have anything but 0 as their RPR.\n",
    "\n",
    "The good score is probably owning to the fact that the 'correct' answer doesn't have to be an entire triple anymore, it only needs to be the correct predicate. **However, the problem mentioned earlier isn't fixed yet.** The following example demonstrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(search(['Jeff_Bezos', 'Amazon.com'], ['created']))\n",
    "print(search(['Jeff_Bezos', 'Amazon.com'], ['owns']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this aside and assuming that the MRR would go up to 0.8 with minor tweaking of the ranking algorithm, it is clear that the main weakness of the new search function is finding the relationship between search terms that don't appear together in any triple in the knowledge graph of the Yago dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
